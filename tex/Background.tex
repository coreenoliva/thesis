\chapter{Background}
The following chapter contains background and theory knowledge which have been applied throughout the thesis. 
\section{MRIs}
The purpose of medical imaging is to obtain information about the anatomy and physiological processes within the body. There are various methods that can be used to capture medical images. One of the most common types are Magnetic Resonance Imaging (MRI). MRIs are popular as they provide detailed images quickly. They are also flexibility in applications as it can capture many different anatomies of the body. \cite{ref:mri_1}
\\[1\baselineskip] 
MRIs are ideal for medical diagnosis and analysis as it has low noise in compared to other types of medical images and are highly detailed. Using the principle of Nuclear Magnetic Resonance, the machine induces the magnetic properties within the nuclei with a magnetic field. The machine then detects these signals and is able to form a 3D voxel based on this. MRIs can do more sophisticated analysis of the body as it is able to differentiate between tissues and show this representation accurately. \cite{ref:mri_2}

\section{Machine Learning}
Machine learning is a method that gives computers the ability to solve solutions by learning from supplied data. Machine learning is getting increasingly important as the amount of data and variety grows. The applications of this method become more feasible as computer power gets more powerful and cheaper and algorithms become more advanced and efficient. 
\\[1\baselineskip]
Machine learning can have either classification or regression types of model and outputs depending on the application and algorithm. Classification outputs can be binary or multi-class. Where binary has two classes and the latter can have multiple. The algorithm will predict what class the data should belong to. Regression output is a continuous output, meaning a numeric value. This is often represented as a percentage or probability.  

\subsection{Types of Machine Learning}
There are various types of machine learning, some include \cite{ref:ml_1}:
\begin{itemize}
	\item Supervised learning
	\item Unsupervised learning
	\item Reinforcement learning
\end{itemize}

Given a scenario with two possible outcomes, supervised learning learns by being supplied with examples for either outcome. The algorithm then learns and analyses the patterns found in the data and provides a prediction. Supervised learning will be the main focus of this thesis.
\\[1\baselineskip]
In contrast, in unsupervised learning, the algorithm is not given any targets or answers. Instead, it aims to find patterns and sequences that are present in the data. An example application for this type of learning is recommendation engines. Based on trends in browsing history for example, the model will be able to make a prediction on things that would be if similar interest.
\\[1\baselineskip]
Reinforcement learning learns through trial and error. It's output is dependent on a sequence of actions which maximise the end goal or reward. Applications which incorporate this type of learning are games, where the agent learns depending on the output of the environment.

\subsection{Overfitting and Underfitting}
The model used to make predictions has to fit the data provided in order to make accurate predictions. There are two spectrum; overfitting and underfitting. The midpoint between the two would be the best case for developing a general and robust model. 
\subsubsection{Overfitting}
Overfitting is when a model is too adapted for the training data that was supplied. When making predictions on unseen data, the model will struggle to make accurate predictions as it is trained to predict accurately on the training data \cite{ref:ml_1}. Overfitting is often due to having too much data to learn from. Overfitting can be mitigated by reducing the amount of training data supplied. 

\subsubsection{Underfitting}
A model that is underfitted is unable to make accurate predictions on the test and training data \cite{ref:ml_1}. This is an indication that this is a poor model as it is not robust enough due to the lack of training data supplied. By increasing the number of training examples and features may counter this issue.   
	
\section{Random Forest}
Random Forest is a machine learning algorithm that can give a classification or regression output. It is compromised of many decision trees - making it a forest.  Random forests are discriminative learners that rely on feature learning in order to conduct its classification or regression. 
\\[1\baselineskip]
In Random Forest, each individual tree grows by adding branches and leaves depending on the complexity of the problem. Each tree is grown to the largest extent possible. The maximum number of trees grown by the Random Forest is a parameter that can be defined. Each tree has an its own individual prediction. The Random Forest then chooses the prediction that has the most votes to be the output of the algorithm. 
\todo[inline]{Benefits and limitations  of random forest}
\subsection{Decision Trees}
Decision trees are a way of mapping out possible decisions and their outcomes in a hierarchical structure. They are composed of internal decision nodes where each node has a test function and the output branches are possible outcomes of this test. The path taken along the branches are dependent on the output decision made at each node. The process starts at the root and repeats until a leaf node is hit. 

\subsection{Training and Testing}
The Random Forest process can be split into two main phases: training and testing. Both of these phases can be either classification or regression. 

\subsubsection{Training}
Training is where the decision trees are developed and grown to become the forest. They are developed based on the training examples and targets provided. Specifically to Random Forest, a portion of the data is left out during training in order to do the importance estimation of the model \cite{ref:rf_1}.  

\subsubsection{Testing}
Testing is where predictions are made based on the model created during the training phase. The aim of this phase is to gain an estimate on the accuracy of the model. This is done by supplying test data and comparing the models' prediction on this data against the appropriate targets. Again, the Random Forest can make classification and regression predictions. The testing data is run down each tree and each individual tree provides a prediction \cite{ref:rf_1}. The final output is determined by the maximum number of trees that had the same prediction. 
 
\section{Features}
Features of a digital image are calculated to give mathematical representation of certain aspects present. This is used to to teach the learning algorithm how to interpret the data supplied. The following features are within the scope of the project. 
\subsection{Intensity}
The intensity of an image is described by the grey-scale level value of each pixel. Where grey-scale ranges from 0 to 255. 
\subsection{GLCM}
\todo[inline]{can i reference matlab}
Grey Level Co-Occurence is a texture feature that considers spatial relationships throughout the image. This is best for texture analysis, which is useful in this scope to determine between bone-like textures and non-bone-like textures. \cite{ref:glcm_1}
\subsection{Gradient}
Gradient accounts for the directional change in intensity within the image. This provides for a more in-depth analysis about the intensities present. \cite{ref:gradient_1} 
\subsection{Entropy}
The entropy feature describes the business and chaos of an image. Areas within the image that are non-flat are said to be busier than flat images as more information is needed to describe this. This feature will help detect shapes within the image. \cite{ref:ent_1}

\section{Superpixels}
Superpixels are a lattice of image patches that are aligned along the border found with an image. They are often used in image processing as pre-processing stages for image segmentation. These superpixel patches are uniform in certain image features such as intensity, colour and texture. This generally makes them originate from the same object within an image. \cite{ref:sp_2}
\\[1\baselineskip]
Their functional aim is to reduce computational load while retaining the same amount of information. The use of superpixel patches reduce the processing required; for example thousands of pixels down to hundreds. This contributes to the reduction in computational load and increasing speed. 
\\[1\baselineskip]
There are various algorithms that implement superpixel segmentation. One within the scope of this thesis is the SLIC algorithm. The SLIC algorithm clusters neighbouring pixels based on colour similarities and proximity within an image. \cite{ref:sp_1}
 
\section{Morphology}
Morphology in image processing are operations conducted on binary images with the use of structural elements. Morphology changes the structure and form of an image and are used to find boundaries and skeletons of an image or could be utilised for thinning or pruning edges. Morphology is a common pre or post processing stage in computer vision problems. However, they are limited to binary and greyscale images. 
\subsubsection{Structural Element}
A structural element defines the output shape of the morphology process. Various aspects of the structural element need to be considered when applying morphology. The shape of the structural element eludes to the edge of the output. The size of the structural element indicate how large the morphology operation will impact the image. Some shapes include:
\begin{itemize}
	\item Disc
	\item Square
	\item Diamond
	\item Sphere
	\item Cube
\end{itemize}
 \cite{ref:morph_3}.
\subsection{Types of Morphology}
There are various types of morphology operations to achieve different outputs depending on the application. The operations are conducted by convolution-type processes of the structural element on the image. The operations can be represented as mathematical equations where A is the image and B is the structural element.

\subsubsection{Dilation}
Dilation is commonly used to extend or expand boundaries found within an image. Dilation of an image can be described by equation below. 
\begin{equation}
A \oplus B = {x | \hat{B}_x \cap A }
\label{eq:dilation}
\end{equation} 
In equation \ref{eq:dilation}, A is added with the reflection of B which is translated along the x axis. This expands the image as the pixels are being overlapped.

\todo[inline]{need to add dilation image} 

\subsubsection{Erosion}
Contrary to dilation, erosion does the opposite. Erosion of an image trims or prunes down the edges found within the image. The equation for erosion is supplied below.
\begin{equation}
A \ominus B = {x | \hat{B}_x \cap A }
\label{eq:erosion}
\end{equation}
The equation \ref{eq:erosion} above,  does the same translation and reflection of structural element, but instead subtracts it from the image. This ensures that the output image is contained within the original image A, trimming the edges of the image. 
\todo[inline]{add erosion diagram}

\subsubsection{Opening}
Opening is the process of combining the erosion operation followed by dilation. This is often used to smooth out the contours of an image. This is done by first eroding out the rough edges with erosion and expanding it back close to its original size with dilation.
\begin{equation}
A \circ B = (A \ominus B) \oplus B
\label{eq:open}
\end{equation}

\todo[inline]{insert diagram}
\subsubsection{Closing}
The closing morphology process conducts the dilation process followed by erosion. This operation is used to fill gaps and eliminate small holes within an image. By dilating the image, this causes small gaps to be filled. The erosion process after will reduce the number of individual small objects such as noise and specks.
\begin{equation}
A \bullet B = (A \oplus B) \ominus B
\label{eq:close}
\end{equation}
\todo[inline]{insert diagram}

\section{Statistics and Analysis}
There is a need to quantize the performance of a model. The performance of the classifier needs to be estimated and analysed in terms of how accurately it can predict the correct classification when given an unseen example.
\\[1\baselineskip]
In classification type outputs, there are four possible outcomes of the results. These are:
\begin{itemize}
	\item True Positive
	\item False Positive
	\item True Negative
	\item False Negative
\end{itemize} 
True positive is when both the prediction result is accurately positive. False positive is when the prediction is positive on a negative example. True negative is when the prediction accurately predicts negative on a negative example. Lastly, false negative is when the prediction is negative on a positive example \cite{ref:ml_1}. 
\\[1\baselineskip]
After determining the number of each type output, the rates can be calculated to provide a further analysis of the performance. These rates are true positive rate and the true negative rate. These are also known as the sensitivity and specificity. 
\begin{equation}
\textrm{Sensitivty} = \frac{\textrm{True Positive}}{\textrm{Total Postives}}
\label{eq:sensitivity}
\end{equation}
\\
\begin{equation}
\textrm{Specificity} = \frac{\textrm{True Negative}}{\textrm{Total Negatives}}
\label{eq:specificity}
\end{equation}
\\
Where sensitivity from equation \ref{eq:sensitivity} shows the percentage that the model is able to predict positives accurately and specificity \ref{eq:specificity} is the probability of correctly predicting negatives \cite{ref:stat_1}. From these rates, the total accuracy can also be calculated. The accuracy represents the percentage of the total samples that were accurately predicted, whether it be positive or negative. 
\\
\begin{equation}
\textrm{Accuracy} = \frac{\textrm{True Postives} + \textrm{True Negatives}}{\textrm{Total Number}}
\end{equation}
\\
Receiver Operating Characteristics (ROC) curves can be produced to visually represent the sensitivity versus specificity \cite{ref:ml_1}. This plot is used to compare varying parameters, which is useful in justifying and selecting optimal parameters.
